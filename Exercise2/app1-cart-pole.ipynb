{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "98b63b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "88478188",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 400  # number of episodes\n",
    "EPS_START = 0.9  # e-greedy threshold start value\n",
    "EPS_END = 0.05  # e-greedy threshold end value\n",
    "EPS_DECAY = 200  # e-greedy threshold decay\n",
    "GAMMA = 0.8  # Q-learning discount factor\n",
    "LR = 0.001  # NN optimizer learning rate\n",
    "HIDDEN_LAYER = 256  # NN hidden layer size\n",
    "BATCH_SIZE = 64  # Q-learning batch size\n",
    "\n",
    "savedStates = []\n",
    "LastRunsRewardSums = [0,0,0,0,0]\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "50254e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, HIDDEN_LAYER)\n",
    "        self.l2 = nn.Linear(HIDDEN_LAYER, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "abeb9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "# env = wrappers.Monitor(env, '.', force=True)\n",
    "\n",
    "model = Network()\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "memory = ReplayMemory(10000)\n",
    "optimizer = optim.Adam(model.parameters(), LR)\n",
    "steps_done = 0\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "561c430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "        \n",
    "    if sample > eps_threshold:\n",
    "        return model(Variable(state).type(FloatTensor)).data.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(2)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "56b82ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_episode(e, environment):\n",
    "    state = environment.reset()\n",
    "    steps = 0\n",
    "    global LastRunsRewardSums\n",
    "    global savedStates\n",
    "    saveData = 180 < sum(LastRunsRewardSums)/len(LastRunsRewardSums)\n",
    "    rewardsum = 0\n",
    "    while True:\n",
    "        environment.render()\n",
    "        action = select_action(FloatTensor([state]))\n",
    "        next_state, reward, done, _ = environment.step(action[0, 0].item() )\n",
    "\n",
    "        # negative reward when attempt ends\n",
    "        if done:\n",
    "            reward = -1\n",
    "\n",
    "        if saveData:\n",
    "            savedStates.append((state.tolist(),action.item()))\n",
    "\n",
    "        memory.push((FloatTensor([state]),\n",
    "                     action,  # action is already a tensor\n",
    "                     FloatTensor([next_state]),\n",
    "                     FloatTensor([reward])))\n",
    "\n",
    "        learn()\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        rewardsum+=reward\n",
    "        if done:\n",
    "            LastRunsRewardSums.append(rewardsum)\n",
    "            LastRunsRewardSums = LastRunsRewardSums[1:6]\n",
    "            print(LastRunsRewardSums)\n",
    "            print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                  .format(e, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "            episode_durations.append(steps)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "465e42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)\n",
    "\n",
    "    batch_state = Variable(torch.cat(batch_state))\n",
    "    batch_action = Variable(torch.cat(batch_action))\n",
    "    batch_reward = Variable(torch.cat(batch_reward))\n",
    "    batch_next_state = Variable(torch.cat(batch_next_state))\n",
    "\n",
    "    # current Q values are estimated by NN for all actions\n",
    "    current_q_values = model(batch_state).gather(1, batch_action)\n",
    "    # expected Q values are estimated from actions which gives maximum Q value\n",
    "    max_next_q_values = model(batch_next_state).detach().max(1)[0]\n",
    "    expected_q_values = batch_reward + (GAMMA * max_next_q_values)\n",
    "    \n",
    "\n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(current_q_values, expected_q_values.view(-1,1))\n",
    "\n",
    "    # backpropagation of loss to NN\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6f01c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.FloatTensor(episode_durations)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eee1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(EPISODES):\n",
    "    run_episode(e, env)\n",
    "\n",
    "print('Complete')\n",
    "# env.render(close=True)\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a967b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "global savedStates\n",
    "outputList = []\n",
    "min = 5\n",
    "for tuple in savedStates:\n",
    "    outputList.append(tuple[0]+[tuple[1]])\n",
    "\n",
    "print(outputList[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d6e8cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "global savedStates\n",
    "global outputList\n",
    "\n",
    "with open(\"output.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(outputList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d0d54367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class CartPoleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        data = np.loadtxt('output.csv', delimiter=\",\", dtype = np.float32)\n",
    "        self.state = torch.from_numpy(data[:,0:-1])\n",
    "        self.action = torch.from_numpy(data[:,-1].astype(int))\n",
    "        self.n_samples = self.state.shape[0]\n",
    "        self.output_dim = len(torch.unique(self.action))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.state[index], self.action[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "843c00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first = True)\n",
    "        #input shape -> batch_size, input_size\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # print(input.size())\n",
    "        h0 = torch.zeros(1,self.hidden_size)\n",
    "        # print(h0.size())\n",
    "        out, h_n = self.rnn(input, h0.detach())\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9ca11f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CartPoleDataset()\n",
    "dataloader = DataLoader(dataset=dataset, batch_size = 10, shuffle = True, num_workers=2)\n",
    "print(dataset.n_samples)\n",
    "input_size = 4\n",
    "hidden_size = 16\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, dataset.output_dim)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "num_epoch = 100\n",
    "\n",
    "print(\"Hi\")\n",
    "for epoc in range(num_epoch):\n",
    "    print(epoc)\n",
    "    epoch_loss = 0\n",
    "    num_correct = 0\n",
    "    for i, (states, actions) in enumerate(dataloader):\n",
    "        #Forward pass\n",
    "        rnn.zero_grad()\n",
    "        \n",
    "        prediction = rnn(states)\n",
    "        print(prediction)\n",
    "        print(actions)\n",
    "        loss = criterion(prediction, actions)\n",
    "\n",
    "        #Backwards pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #Update\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b06ebfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    return torch.argmax(rnn(torch.from_numpy(np.array([state])))).item()\n",
    "\n",
    "rewardSum = 0\n",
    "env2 = gym.make('CartPole-v0')\n",
    "state = env2.reset()\n",
    "while True:\n",
    "    state, reward, done, _ = env2.step(policy(state))\n",
    "\n",
    "    rewardSum+=reward\n",
    "    env2.render()\n",
    "    if done:\n",
    "        print(rewardSum)\n",
    "        rewardSum = 0\n",
    "        state = env2.reset()\n",
    "print('Complete')\n",
    "# env.render(close=True)\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236411a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0d8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
